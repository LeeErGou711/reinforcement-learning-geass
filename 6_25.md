


# # theory lecture 3: reinforcement learning basics 
## introduction
## MDP: markov decision process
### markov process
### markov decision process
MDP can be described by a 5-tuple:
- S: the state space
- A: the action space
- $\{P_{sa}\}$: the environment transition probability function
- $\gamma$: the unnormalized discount factor
- R: reward function

MDP procedure: start from state $s_0$; choose an action $a_0\in A$; get reward $R(s_0, a_0)$, randomly tranform to next state $s_1$; repeat the steps.
the total reward is $R(s_0,a_0)+\gamma R(s_1,a_1)+\gamma^2 R(s_2,a_2)+...$

given a dynamic environment, different policy $\pi$ leads to different $\{P_{sa}\}$. to describe this fact, we introduce the occupancy measure.

occupancy measure $\rho^\pi(s,a)$:  
$$\rho^\pi(s,a) = \mathbb{E}_{a\sim s_,s'\sim p(s,a)}[\sum_{t=0}^T\gamma^t p(s_t = s, a_t = a)]$$
we have two theorems about occupancy measure: 
theorem 1: given two policies $\pi_1$ and $\pi_2$, we have $$\rho^{\pi_1} = \rho^{\pi_2}\quad iff \quad \pi_1 = \pi_2$$
theorem 2: given a occupancy measure $\rho$, the only policy which can generate $\rho$ is: $$\pi_{\rho} = \frac{\rho(s,a)}{\sum_{a'}\rho(s,a')}$$
state occupancy measure $\rho^{\pi}(s)$: $$\rho^{\pi}(s) = \sum_{a'}\rho^\pi(s,a')$$

the accumulated reward $V(\pi)$: $$V(\pi) = \sum_{s,a}\rho^\pi(s,a)R(s,a)$$
we can prove that $\sum_{\pi}V(\pi) = \frac{1}{\gamma}$
## dynamic programming
agent: 

the purpose of agent: choose the action that can maximize the accumulated reward $\mathbb{E}[R(s_0)+\gamma R(s_1)+\gamma^2 R(s_2)+...]$

$\gamma\in [0,1]$ is a factor which lets the agent focus more on current reward than future reward

policy $\pi$: is a function $\pi(s):S\rightarrow A$, which denotes that in state s the agent takes the action $a = \pi(s)$
we define the value functions for $\pi$: 

state value function: $$V^{\pi}(s) = \mathbb{E}[R(s_0)+\gamma R(s_1)+\gamma^2R(s_2)+...|s_0 = s,\pi]$$
action value function: $$Q^\pi(s,a) = \mathbb{E}[R(s_0,a_0)+\gamma R(s_1,a_1)+\gamma^2 R(s_2,a_2)+...|s_0 = s,a_0 = a,\pi] = R(s,a)+\gamma\sum_{s'\in S}P_{s,a = \pi(s)}(s')V^{\pi}(s')$$
we have the Bellman equality: $$V^\pi(s) = R(s) + \gamma\sum_{s'\in S}P_{s\pi(s)}(s')V^\pi(s')$$which describes the relationship between $V(s)$ and $V(s')$
### optimal value function
for a state s, the optimal value function is: $$V^*(s) = \max_\pi V^\pi(s)$$
the Bellman equality of the optimal value function is $$V^*(s) = R(s)+\max_{a\in A}\gamma\sum_{s'\in S}P_{sa}(s')V^*(s')$$
the optimal policy is $$\pi^*(s) = \argmax_{a\in A}\sum_{s'\in S}P_{sa} (s')V^*(s')$$ 
given a state $s$ and a policy $\pi$, we have $$V^*(s) = V^{\pi^*}(s)\geq V^\pi(s)$$
we can update the optimal value function and the optimal policy iterally:

value iteration: for a MDP with finite action space $A$ and finite state space $S$:
- initialize $V(s) = 0$
- repeat the process untill converging:
	- for every state $s$, update $V(s) = R(s)+\max_{a\in A}\gamma\sum_{s'\in S}P_{sa}(s')V(s')$

policy iteration: for a MDP with finite action space $A$ and finite state space $S$:
- initialize policy $\pi$;
- repeat the process untill converging:
	- let $V = V^\pi$
	- for every state $s$, update $\pi(s) =\argmax_{a\in A} \sum_{s'\in S}P_{sa}(s')V(s')$
### policy evaluation and policy improvement
policy estimation: 
$$V^\pi(s) = \mathbb{E}[R(s_0,a_0)+\gamma R(s_1,a_1)+...|s_0 = s,\pi] = \mathbb{E}_{a\sim \pi(s)}[R(s,a)+\gamma\sum_{s'\in S}P_{s\pi(s)}(s)V^{\pi}(s')] = \mathbb{E}_{a\sim\pi(s)}[Q^\pi(s,a)]$$

$$Q^\pi(s,a) = \mathbb{E}[R(s_0,a_0)+\gamma R(s_1,a_1)+\gamma^2 R(s_2,a_2)+...|s_0 = s,a_0 = a,\pi] = R(s,a)+\gamma\sum_{s'\in S}P_{s\pi(s)}(s')V^\pi(s')$$

policy improvement: for two policies $\pi$ an $\pi'$, we say $\pi'$ is a policy improvement of $\pi$ if for any state $s$, we have $$Q^\pi(s,\pi'(s))\geq V^\pi(s)$$
policy improvement theorem: given two policies $\pi,\pi'$, if exists a state $s$ s.t. $Q^\pi(s,\pi'(s))\geq V^\pi(s)$, then $V^{\pi'}(s)\geq V^\pi(s)$

using this theorem, we know that if $s$ is a state and $Q^\pi(s,\pi'(s))\geq V^\pi(s)$, then $\pi'$ is a policy improvement of $\pi$ 
## value function estimation
when the state transition $P_{sa}$ and reward function $R$ are not known, we want to estimate the value function.
###  Monte Carlo method
recall the return $$G_t = R_{t+1}+\gamma R_{t+2}+...+\gamma^{T-1}R_T$$ and the value function is $$V^\pi(s) = \mathbb{E}[R(s_0)+\gamma R(s_1)+...|s_0 = s,\pi] = \mathbb{E}[G_t|s_t = s,\pi]\cong \frac{1}{N}\sum_{i=1}^N G^{(i)}_t$$
we update $V(s)$ after every episode. In every episode, we visit a state $s$ after every stride time $t$ and have:
- $N(s)\leftarrow N(s)+1$
- accumulated reward$S(s)\leftarrow S(s)+G_t$
- reward $V(s) = S(s)/N(s)$

we have $V(s)\rightarrow V^{\pi}(s)$ as $N(s)\rightarrow \infty$ by large number principle

after one episode, we get new $S_t$ and $G_t$ and for every state $S_t$ and the accumulated reward $G_t$, we have $$V(S_t) \cong \frac{1}{N}\sum_{i=1}^N G_{t}^{(i)}$$we update $V(S_t)$ by $V(S_t)+\alpha(G_t-V(S_t))$

note that Monte Carlo method learn from complete episodes: without using bootstrapping

Monte Carlo method can only apply to finite MDP.
### Temporal difference method
$V(S_t) \leftarrow V(S_t)+\alpha (R_{t+1}+\gamma V(S_{t+1})-V(S_t))$
note that TD does not need complete episode

$R_{t+1}+\gamma V(S_{t+1})$ is a biased estimation of $V^{\pi}(S_t)$, so TD is a biased estimation method.
### compare MC and TD: 
- TD is a biased estimation, MC is an unbiased estimation
- MC has larger variance than TD
- TD can handle infinite lengh case
## Model-free control method
### SARSA (state-action-reward-state-action)
for every time $t$, we have $(s,a)\rightarrow (s',a')$. SARSA updates the function $Q(s,a)$ by $$Q(s,a)\leftarrow Q(s,a)+\alpha(r+\gamma Q(s',a')-Q(s,a))$$
we can use $\epsilon$-greedy to get $a'$ in the state $s'$
### Q-learning
with state $s_t$, action $a_t$, and reward $r_{t+1}$, we move to next state $s_{t+1}$, then we adopt a different way to get $a_{t+1}$: $$Q(s_t,a_t)\leftarrow Q(s_t,a_t)+\alpha(r_{t+1}+\gamma \max_{a'}Q(s_{t+1},a')-Q(s_t,a_t))$$

Q-learning is a off-policy algorithm, while SARSA is a on-policy algorithm
## parameterized vaue function
## strategy gradient 
